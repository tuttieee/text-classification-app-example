{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Livedoorニュースコーパスを使った実験\n",
    "\n",
    "http://www.rondhuit.com/download.html#ldcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import join, normpath, isdir\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rx_filename = re.compile('^.*\\d+\\.txt$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dokujo-tsushin\n",
      "it-life-hack\n",
      "kaden-channel\n",
      "livedoor-homme\n",
      "movie-enter\n",
      "peachy\n",
      "smax\n",
      "sports-watch\n",
      "topic-news\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "labels = np.array([], dtype=np.uint8)\n",
    "label_names = []\n",
    "\n",
    "datadir = normpath('../data/livedoornews')\n",
    "subdirs = listdir(datadir)\n",
    "label = 0\n",
    "for subdir in subdirs:\n",
    "    subdir_path = join(datadir, subdir)\n",
    "    if not isdir(subdir_path):\n",
    "        continue\n",
    "    \n",
    "    texts_of_this_label = []\n",
    "    textfiles = listdir(subdir_path)\n",
    "    for j, textfile in enumerate(textfiles):\n",
    "        if not rx_filename.match(textfile):\n",
    "            continue\n",
    "            \n",
    "        textfile_path = join(subdir_path, textfile)\n",
    "        \n",
    "        with open(textfile_path, 'r') as f:\n",
    "            texts_of_this_label.append(f.read())\n",
    "            \n",
    "    \n",
    "    texts.extend(texts_of_this_label)\n",
    "    labels = np.hstack((labels, np.ones(len(texts_of_this_label), dtype=np.uint8) * label))\n",
    "    label_names.append(subdir)\n",
    "    \n",
    "    label += 1\n",
    "    print subdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import neologdn\n",
    "import re\n",
    "\n",
    "class MeCabTokenizer(object):\n",
    "    def __init__(self, dicpath=None):\n",
    "        if dicpath is not None:\n",
    "            dicpath = ' -d ' + dicpath\n",
    "        else:\n",
    "            dicpath = ''\n",
    "            \n",
    "        self.tagger = MeCab.Tagger('-Ochasen' + dicpath)\n",
    "        self.rx_url = re.compile('https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-]+')\n",
    "    \n",
    "    def __call__(self, text):\n",
    "        return self.tokenize(text)\n",
    "    \n",
    "    def normalize_word(self, text):\n",
    "        text = self.rx_url.sub('URLTOKEN', text)\n",
    "        return text\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        input_type = type(text)\n",
    "        bow = []\n",
    "        \n",
    "        #text = neologdn.normalize(text.decode('utf8')).encode('utf8')\n",
    "        if input_type == str:\n",
    "            text = text.decode('utf8')\n",
    "        \n",
    "        text = neologdn.normalize(text).encode('utf8')\n",
    "        text = self.normalize_word(text)\n",
    "        \n",
    "        node = self.tagger.parseToNode(text)\n",
    "        while node:\n",
    "            feature = node.feature.split(',')\n",
    "            \n",
    "            if feature[0] == 'BOS/EOS':\n",
    "                node = node.next\n",
    "                continue\n",
    "            \n",
    "            if feature[0] in ['名詞', '動詞', '形容詞', '副詞', '連体詞']:\n",
    "                word = feature[6] if feature[6] != '*' else node.surface\n",
    "                \n",
    "                if feature[1] == '数':\n",
    "                    word = 'NUMTOKEN'\n",
    "                \n",
    "                if feature[1] == '接尾':\n",
    "                    node = node.next\n",
    "                    continue\n",
    "                \n",
    "                if input_type == unicode:\n",
    "                    word = word.decode('utf8')\n",
    "                \n",
    "                bow.append(word)\n",
    "                \n",
    "            node = node.next\n",
    "        \n",
    "        return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今日\n",
      "焼肉\n",
      "NUMTOKEN\n",
      "食べる\n"
     ]
    }
   ],
   "source": [
    "tokenizer = MeCabTokenizer('/usr/local/lib/mecab/dic/mecab-ipadic-neologd/')\n",
    "for w in tokenizer('今日は焼肉が１０食食べたいな'):\n",
    "    print w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer(analyzer=<__main__.MeCabTokenizer object at 0x139bfee90>,\n",
       "        binary=False, decode_error=u'strict', dtype=<type 'numpy.int64'>,\n",
       "        encoding=u'utf-8', input=u'content', lowercase=True, max_df=1.0,\n",
       "        max_features=None, min_df=1, ngram_range=(1, 1), norm=...n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(len(texts))\n",
    "X = pd.Series(texts)[indices]\n",
    "Y = labels[indices]\n",
    "\n",
    "n_test = 500\n",
    "X_test = X[:n_test]\n",
    "X_train = X[n_test:]\n",
    "Y_test = Y[:n_test]\n",
    "Y_train = Y[n_test:]\n",
    "\n",
    "tokenizer = MeCabTokenizer()\n",
    "vectorizer = TfidfVectorizer(analyzer=tokenizer)\n",
    "classifier = RandomForestClassifier(n_estimators=120)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        ('tfidf', vectorizer),\n",
    "        ('classifier', classifier),\n",
    "    ])\n",
    "pipeline.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.936\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "Y_pred = pipeline.predict(X_test)\n",
    "print accuracy_score(Y_pred, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63515\n"
     ]
    }
   ],
   "source": [
    "print len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定性評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label:  it-life-hack\n",
      "=============\n",
      "dokujo-tsushin 0.0833333333333\n",
      "it-life-hack 0.325\n",
      "kaden-channel 0.2\n",
      "livedoor-homme 0.133333333333\n",
      "movie-enter 0.0166666666667\n",
      "peachy 0.0916666666667\n",
      "smax 0.116666666667\n",
      "sports-watch 0.0333333333333\n",
      "topic-news 0.0\n"
     ]
    }
   ],
   "source": [
    "test_input = \"\"\"\n",
    "アップルは米国で発表会を行い同社のスマートフォンの新モデルとなる「iPhone 7」および、大型サイズの「iPhone 5 plus」を発表した。\n",
    "\n",
    "画面サイズはiPhone 7が4.7インチ、iPhone 7 Plusが5.5インチと前モデルから変わっていない。カラバリとして新色として「ジェットブラック」と呼ばれるテカテカ系の黒（いわゆるピアノブラック）、これにテカテカしてないつや消しブラックの「ブラック」、黒だけで2色が追加され全部で5色のバリエーションとなっている。\n",
    "\n",
    "■予約開始は明日から！\n",
    "予約開始日は9月9日からとなっている。出荷は9月16日。価格はiPhone 7が7万2,800円（税別）、iPhone 7 Plusが8万5,800円（同）からとなっている。\n",
    "\n",
    "今回の最大の目玉となるのがFeliCa対応だ。おサイフケータイ非対応のためiPhone購入をあきらめていた人たちにとって新規導入の追い風になるのは間違いない。これまでiPhoneシリーズを一度も購入したことのない筆者でも、FeliCa対応なら買ってもいいかなと思ったくらい。\n",
    "\n",
    "■防水・防塵性能に高機能カメラ\n",
    "大画面モデルとなるiPhone 7 Plusには、予想されていた通り2レンズ＆センサー仕様のメインカメラが搭載されてきた。\n",
    "\n",
    "iPhone 7はシングルレンズ仕様ながらも明るさがF1.8となる明るいレンズでストロボの使用を極力防いでくれるようになった。画素数は従来と同じながら、ノイズを極力減らすDTI（Deep Trench Isolation）構造採用によって画質の向上を図っている。光学手ブレ補正機能が搭載されたのもうれしい。\n",
    "\n",
    "事前の噂通り本体が防水・防じんとなり、スピーカーがステレオ化し、3.5mmヘッドフォンジャックが廃止されているのも確認できた。水没iPhoneの悲劇は、これまで以上に発生しにくくなるというわけだ。\n",
    "\n",
    "■新型画像エンジンにより処理も高速化\n",
    "画像処理エンジンが新型になり高速化している点も見逃せない。実にスループットは従来比2倍を実現。オートフォーカスが高速化されたことで、シャッターチャンスを逃さない。ホワイトバランスの精度の向上によってよりリアリティの高い発色を実現している。LEDフラッシュ（ライト）は4灯となり、1.5倍明るくなっている。\n",
    "\n",
    "日本でiPhoneの売れ行きが落ちていると言われていたところに今回の7および7 Plus登場。FeliCa対応などは、明らかに日本市場にターゲットを絞った新機能であり、アップルが日本市場を重視し、ユーザーニーズを押さえにきていることがわかると言えるだろう。\n",
    "\"\"\"\n",
    "\n",
    "y_pred_proba = pipeline.predict_proba([test_input])[0]\n",
    "y_pred = np.argmax(y_pred_proba)\n",
    "print 'Predicted label: ', label_names[y_pred]\n",
    "print '============='\n",
    "for label_name, proba in zip(label_names, y_pred_proba):\n",
    "    print label_name, proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
